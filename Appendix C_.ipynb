{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "error",
     "timestamp": 1751433010748,
     "user": {
      "displayName": "Antonio Gulli",
      "userId": "17769953396342459304"
     },
     "user_tz": -120
    },
    "id": "NhfdOggSqMrH",
    "outputId": "b9dff763-9cf1-42fa-c793-f731632f3eae"
   },
   "outputs": [],
   "source": [
    "# A simple LCEL chain conceptually\n",
    "# (This is not runnable code, just illustrates the flow)\n",
    "\n",
    "# chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCQ5d2idgitI"
   },
   "outputs": [],
   "source": [
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    story: str\n",
    "    poem: str\n",
    "    combined_output: str\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def call_llm_1(state: State):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "\n",
    "def call_llm_2(state: State):\n",
    "    \"\"\"Second LLM call to generate story\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a story about {state['topic']}\")\n",
    "    return {\"story\": msg.content}\n",
    "\n",
    "\n",
    "def call_llm_3(state: State):\n",
    "    \"\"\"Third LLM call to generate poem\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n",
    "    return {\"poem\": msg.content}\n",
    "\n",
    "\n",
    "def aggregator(state: State):\n",
    "    \"\"\"Combine the joke and story into a single output\"\"\"\n",
    "\n",
    "    combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n",
    "    combined += f\"STORY:\\n{state['story']}\\n\\n\"\n",
    "    combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n",
    "    combined += f\"POEM:\\n{state['poem']}\"\n",
    "    return {\"combined_output\": combined}\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "parallel_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n",
    "parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n",
    "parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n",
    "parallel_builder.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "parallel_builder.add_edge(START, \"call_llm_1\")\n",
    "parallel_builder.add_edge(START, \"call_llm_2\")\n",
    "parallel_builder.add_edge(START, \"call_llm_3\")\n",
    "parallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"aggregator\", END)\n",
    "parallel_workflow = parallel_builder.compile()\n",
    "\n",
    "# Show workflow\n",
    "display(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "state = parallel_workflow.invoke({\"topic\": \"cats\"})\n",
    "print(state[\"combined_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1751434280137,
     "user": {
      "displayName": "Antonio Gulli",
      "userId": "17769953396342459304"
     },
     "user_tz": -120
    },
    "id": "vaHmwKWLmzgW",
    "outputId": "ddd8ea58-432a-488c-ad58-5540e186a4b0"
   },
   "outputs": [],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.tools import google_Search\n",
    "\n",
    "dice_agent = LlmAgent(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    name=\"question_answer_agent\",\n",
    "    description=\"A helpful assistant agent that can answer questions.\",\n",
    "    instruction=\"\"\"Respond to the query using google search\"\"\",\n",
    "    tools=[google_search],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpbk9C66njvd"
   },
   "outputs": [],
   "source": [
    "@crew\n",
    "def crew(self) -> Crew:\n",
    "    \"\"\"Creates the research crew\"\"\"\n",
    "    return Crew(\n",
    "      agents=self.agents,\n",
    "      tasks=self.tasks,\n",
    "      process=Process.sequential,\n",
    "      verbose=True,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "15XCzDOvBhIQaZ__xkvruf5sP9OznAbK9",
     "timestamp": 1749895601577
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
